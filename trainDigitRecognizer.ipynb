{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "dataDir = \"E:/Work/NonoGram/\"\n",
    "import json\n",
    "import keras\n",
    "import keras.utils\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import createDigitImage\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_output_dir = \"model_evaluation_result\"\n",
    "def evaluate_tests(model, test_data, test_labels):\n",
    "    empty_test_dir()\n",
    "    predictions = model.predict(test_data)\n",
    "    for i in range(0, len(predictions)):\n",
    "        predicted_digit = np.argmax(predictions[i])\n",
    "        if (predicted_digit != test_labels[i]):\n",
    "            image = np.reshape(test_data[i], (28, 28, 1))*255\n",
    "            cv2.imwrite(os.path.join(evaluate_output_dir, f\"{predicted_digit}_{test_labels[i]}_{i}.png\"), image)\n",
    "\n",
    "def empty_test_dir():\n",
    "    if os.path.exists(evaluate_output_dir):\n",
    "        for filename in os.listdir(evaluate_output_dir):\n",
    "            file_path = os.path.join(evaluate_output_dir, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "    os.makedirs(evaluate_output_dir, exist_ok=True)                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, output_dir='error_logs'):\n",
    "        print(\"start\")\n",
    "        self.failed_epochs = []  # List to store failed cases\n",
    "        self.output_dir = output_dir\n",
    "        self.file_path = os.path.join(self.output_dir, 'failed_epochs.json')\n",
    "\n",
    "        # Clean the directory if it exists (remove old logs)\n",
    "        if os.path.exists(self.output_dir):\n",
    "            for filename in os.listdir(self.output_dir):\n",
    "                file_path = os.path.join(self.output_dir, filename)\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.remove(file_path)\n",
    "\n",
    "        # Ensure the subdirectory exists\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Check for specific conditions (e.g., loss or accuracy being None)\n",
    "        if logs is None or logs.get('loss') is None or logs.get('accuracy') is None:\n",
    "            failed_case = {\n",
    "                'epoch': epoch,\n",
    "                'loss': logs.get('loss'),\n",
    "                'accuracy': logs.get('accuracy')\n",
    "            }\n",
    "            self.failed_epochs.append(failed_case)\n",
    "            print(f\"Epoch {epoch} failed. Loss: {logs.get('loss')}, Accuracy: {logs.get('accuracy')}\")\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        print(\"end\")\n",
    "        with open(self.file_path, 'w') as f:\n",
    "            json.dump(self.failed_epochs, f, indent=4)\n",
    "        with open(os.path.join(self.output_dir,'hoi.txt'), 'w') as g:\n",
    "            g.write(\"aha\")\n",
    "\n",
    "    def get_failed_epochs(self):\n",
    "        return self.failed_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 15000 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 6s 59us/step - loss: 0.3979 - accuracy: 0.8831 - val_loss: 0.0954 - val_accuracy: 0.9702\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 6s 56us/step - loss: 0.1385 - accuracy: 0.9626 - val_loss: 0.0583 - val_accuracy: 0.9826\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 6s 56us/step - loss: 0.0933 - accuracy: 0.9745 - val_loss: 0.0364 - val_accuracy: 0.9885\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 6s 57us/step - loss: 0.0528 - accuracy: 0.9839 - val_loss: 0.0295 - val_accuracy: 0.9907\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 7s 66us/step - loss: 0.0827 - accuracy: 0.9812 - val_loss: 0.0264 - val_accuracy: 0.9907\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 6s 63us/step - loss: 0.0673 - accuracy: 0.9842 - val_loss: 0.0294 - val_accuracy: 0.9899\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 6s 63us/step - loss: 0.0367 - accuracy: 0.9886 - val_loss: 0.0229 - val_accuracy: 0.9919\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 6s 62us/step - loss: 0.0314 - accuracy: 0.9901 - val_loss: 0.0204 - val_accuracy: 0.9929\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 6s 62us/step - loss: 0.0292 - accuracy: 0.9909 - val_loss: 0.0193 - val_accuracy: 0.9932\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 6s 59us/step - loss: 0.0723 - accuracy: 0.9874 - val_loss: 0.0213 - val_accuracy: 0.9927\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def combine_data(data_set_1, data_set_2):\n",
    "    (train_data, train_labels, test_data, test_labels) = data_set_1\n",
    "    (extra_train_data, extra_train_labels, extra_test_data, extra_test_labels) = data_set_2\n",
    "\n",
    "    train_data = np.concatenate([train_data, extra_train_data], axis=0)\n",
    "    train_labels = np.concatenate([train_labels, extra_train_labels], axis=0)\n",
    "    test_data = np.concatenate([test_data, extra_test_data], axis=0)\n",
    "    test_labels = np.concatenate([test_labels, extra_test_labels], axis=0)\n",
    "    return (train_data, train_labels, test_data, test_labels)\n",
    "\n",
    "def load_data_from_mnist():\n",
    "    (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "    train_data = train_data.astype('float32') / 255\n",
    "    test_data = test_data.astype('float32') / 255\n",
    "    train_data = np.expand_dims(train_data, axis=-1)\n",
    "    test_data = np.expand_dims(test_data, axis=-1)\n",
    "    return (train_data, train_labels, test_data, test_labels)\n",
    "\n",
    "def create_data_from_fonts(train_count, test_count):\n",
    "    extra_train_data = [createDigitImage.create_random_digit_image() for i in range(0, train_count)]\n",
    "    extra_test_data = [createDigitImage.create_random_digit_image() for i in range(0, test_count)]\n",
    "\n",
    "    train_data = np.reshape([data for (data, label) in extra_train_data], (train_count, 28, 28, 1))\n",
    "    train_labels = [label for (data, label) in extra_train_data]\n",
    "    test_data = np.reshape([data for (data, label) in extra_test_data], (test_count, 28, 28, 1))\n",
    "    test_labels = [label for (data, label) in extra_test_data]\n",
    "    return (train_data, train_labels, test_data, test_labels)\n",
    "\n",
    "\n",
    "# Load the MNIST dataset from OpenCV (this loads pre-trained data)\n",
    "# This is a convenient way to quickly get started with digit recognition.\n",
    "def createDigitRecognizer():\n",
    "    #(train_data, train_labels, test_data, test_labels) = load_data_from_mnist()\n",
    "    #(train_data, train_labels, test_data, test_labels) = create_data_from_fonts(train_count = 40000, test_count = 2000)\n",
    "    #(train_data, train_labels, test_data, test_labels) = create_data_from_fonts(train_count = 2000, test_count = 1000)\n",
    "    (train_data, train_labels, test_data, test_labels) = combine_data(load_data_from_mnist(), create_data_from_fonts(40000, 5000))\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        #keras.layers.BatchNormalization(),\n",
    "        keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        #keras.layers.BatchNormalization(),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        #keras.layers.BatchNormalization(),\n",
    "        #keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(train_data, tf.keras.utils.to_categorical(train_labels), epochs=10, batch_size=64, validation_data=(test_data, tf.keras.utils.to_categorical(test_labels)))\n",
    "    model.evaluate(test_data,  tf.keras.utils.to_categorical(test_labels), verbose=2)\n",
    "    evaluate_tests(model, test_data, test_labels)\n",
    "\n",
    "    return model\n",
    "\n",
    "import os\n",
    "\n",
    "#import keras.saving\n",
    "digitRecognizerModelFileName = f\"{dataDir}/digitRecognizerMnist.keras\"\n",
    "digitRecognizer = createDigitRecognizer()\n",
    "digitRecognizer.save(f\"{dataDir}/digitRecognizerMnist.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
